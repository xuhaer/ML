{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习100天——第1天：数据预处理（Data Preprocessing）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一步：导入需要的库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第二步：导入数据集\n",
    "我们使用Pandas的read_csv方法读取本地csv文件为一个数据帧。然后，从数据帧中制作自变量和因变量的矩阵和向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country   Age   Salary Purchased\n",
      "0   France  44.0  72000.0        No\n",
      "1    Spain  27.0  48000.0       Yes\n",
      "2  Germany  30.0  54000.0        No\n",
      "3    Spain  38.0  61000.0        No\n",
      "4  Germany  40.0      NaN       Yes\n",
      "\n",
      "the shape of dataset: (10, 4)\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('../datasets/Data.csv')\n",
    "X = dataset.iloc[ : , :-1].values\n",
    "Y = dataset.iloc[ : , 3].values\n",
    "print(dataset.head());print()\n",
    "print('the shape of dataset: {}'.format(dataset.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第三步：处理丢失数据\n",
    "我们得到的数据很少是完整的。数据可能因为各种原因丢失，为了不降低机器学习模型的性能，需要处理数据。我们可以用整列的平均值或中间值替换丢失的数据。我们用sklearn.preprocessing库中的Imputer类完成这项任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "(10, 3)\n",
      "---------------------\n",
      "Step 3: Handling the missing data Another Way:Use pandas\n",
      "X\n",
      "(10, 3)\n",
      "[['France' 44.0 72000.0]\n",
      " ['Spain' 27.0 48000.0]\n",
      " ['Germany' 30.0 54000.0]\n",
      " ['Spain' 38.0 61000.0]\n",
      " ['Germany' 40.0 63777.8]\n",
      " ['France' 35.0 58000.0]\n",
      " ['Spain' 38.8 52000.0]\n",
      " ['France' 48.0 79000.0]\n",
      " ['Germany' 50.0 83000.0]\n",
      " ['France' 37.0 67000.0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(missing_values=\"NaN\", strategy=\"mean\", axis=0) \n",
    "imputer = imputer.fit(X[ : , 1:3])\n",
    "X[ : , 1:3] = imputer.transform(X[ : , 1:3])\n",
    "print(\"X\")\n",
    "print(X.shape)\n",
    "\n",
    "#Step 3: Handling the missing data Another Way:Use pandas\n",
    "X = dataset.iloc[ : , :-1].fillna(dataset.iloc[ : , 1:-1].mean()).round(decimals=1).values\n",
    "print(\"---------------------\")\n",
    "print(\"Step 3: Handling the missing data Another Way:Use pandas\")\n",
    "print(\"X\")\n",
    "print(X.shape)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第四步：解析分类数据\n",
    "分类数据指的是含有标签值而不是数字值的变量。取值范围通常是固定的。例如\"Yes\"和\"No\"不能用于模型的数学计算，所以需要解析成数字。为实现这一功能，我们从sklearn.preprocessing库导入LabelEncoder类。Day 4 莺尾花的那个模型就处理了'str'类型的变量\n",
    "### why?\n",
    "Many machine learning algorithms cannot operate on label data directly. \n",
    "<br>\n",
    "They require all input variables and output variables to be numeric.\n",
    "How to Convert Categorical Data to Numerical Data?\n",
    "<br>This involves two steps:\n",
    "* Integer Encoding\n",
    "* One-Hot Encoding:For categorical variables where no such ordinal relationship exists, the integer encoding is not enough.\n",
    "\n",
    "使用one hot encoding 把categorical data转化为二进制。\n",
    "每个特征用一个二进制数字来表示的方法就是one-hot encoding。\n",
    "该方法将每个具有n个可能的分类特征转换成n个二元特征，且只有一个特征值有效。\n",
    "<br>因country有3个可能的分类特征，所以以下OneHotEncoder把categorical data转化为了3个二元特征[[1,00],[0,0,1],[0,1,0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 4: Encoding categorical data\n",
      "X\n",
      "[[1.00000e+00 0.00000e+00 0.00000e+00 4.40000e+01 7.20000e+04]\n",
      " [0.00000e+00 0.00000e+00 1.00000e+00 2.70000e+01 4.80000e+04]\n",
      " [0.00000e+00 1.00000e+00 0.00000e+00 3.00000e+01 5.40000e+04]\n",
      " [0.00000e+00 0.00000e+00 1.00000e+00 3.80000e+01 6.10000e+04]\n",
      " [0.00000e+00 1.00000e+00 0.00000e+00 4.00000e+01 6.37778e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 3.50000e+01 5.80000e+04]\n",
      " [0.00000e+00 0.00000e+00 1.00000e+00 3.88000e+01 5.20000e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 4.80000e+01 7.90000e+04]\n",
      " [0.00000e+00 1.00000e+00 0.00000e+00 5.00000e+01 8.30000e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 3.70000e+01 6.70000e+04]]\n",
      "(10, 5)\n",
      "Y\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X = LabelEncoder()\n",
    "X[ : , 0] = labelencoder_X.fit_transform(X[ : , 0]) #country\n",
    "#Creating a dummy variable\n",
    "onehotencoder = OneHotEncoder(categorical_features=[0])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "labelencoder_Y = LabelEncoder()\n",
    "Y =  labelencoder_Y.fit_transform(Y)\n",
    "print(\"---------------------\")\n",
    "print(\"Step 4: Encoding categorical data\")\n",
    "print(\"X\")\n",
    "print(X)\n",
    "print(X.shape)\n",
    "print(\"Y\")\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第五步：拆分数据集为测试集合和训练集合\n",
    "把数据集拆分成两个：一个是用来训练模型的训练集合，另一个是用来验证模型的测试集合。两者比例一般是80:20。我们导入sklearn.model_selection库中的train_test_split()方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 5: Splitting the datasets into training sets and Test sets\n",
      "X_train\n",
      "[[0.00000e+00 1.00000e+00 0.00000e+00 4.00000e+01 6.37778e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 3.70000e+01 6.70000e+04]\n",
      " [0.00000e+00 0.00000e+00 1.00000e+00 2.70000e+01 4.80000e+04]\n",
      " [0.00000e+00 0.00000e+00 1.00000e+00 3.88000e+01 5.20000e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 4.80000e+01 7.90000e+04]\n",
      " [0.00000e+00 0.00000e+00 1.00000e+00 3.80000e+01 6.10000e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 4.40000e+01 7.20000e+04]\n",
      " [1.00000e+00 0.00000e+00 0.00000e+00 3.50000e+01 5.80000e+04]]\n",
      "X_test\n",
      "[[0.0e+00 1.0e+00 0.0e+00 3.0e+01 5.4e+04]\n",
      " [0.0e+00 1.0e+00 0.0e+00 5.0e+01 8.3e+04]]\n",
      "Y_train\n",
      "[1 1 1 0 1 0 0 1]\n",
      "Y_test\n",
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.cross_validation import train_test_split \n",
    "# sklearn.cross_validation will be removed in 0.20\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( X , Y , test_size=0.2, random_state=0)\n",
    "print(\"---------------------\")\n",
    "print(\"Step 5: Splitting the datasets into training sets and Test sets\")\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "print(\"X_test\")\n",
    "print(X_test)\n",
    "print(\"Y_train\")\n",
    "print(Y_train)\n",
    "print(\"Y_test\")\n",
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第六步：特征量化\n",
    "大部分模型算法使用两点间的欧氏距离表示，但此特征在幅度、单位和范围姿态问题上变化很大。在距离计算中，高幅度的特征比低幅度特征权重更大。可用特征标准化或Z值归一化解决。导入sklearn.preprocessing库的StandardScalar类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "Step 6: Feature Scaling\n",
      "X_train\n",
      "[[-1.          2.64575131 -0.77459667  0.26258245  0.12381682]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.25397319  0.46175601]\n",
      " [-1.         -0.37796447  1.29099445 -1.97582532 -1.53093364]\n",
      " [-1.         -0.37796447  1.29099445  0.05596019 -1.11142003]\n",
      " [ 1.         -0.37796447 -0.77459667  1.64006416  1.72029684]\n",
      " [-1.         -0.37796447  1.29099445 -0.08178798 -0.16751441]\n",
      " [ 1.         -0.37796447 -0.77459667  0.9513233   0.98614802]\n",
      " [ 1.         -0.37796447 -0.77459667 -0.59834362 -0.48214962]]\n",
      "X_test\n",
      "[[ 0.  0.  0. -1. -1.]\n",
      " [ 0.  0.  0.  1.  1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_X = StandardScaler()\n",
    "X_train = sc_X.fit_transform(X_train)\n",
    "X_test = sc_X.fit_transform(X_test)\n",
    "print(\"---------------------\")\n",
    "print(\"Step 6: Feature Scaling\")\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "print(\"X_test\")\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>完整的项目请前往Github项目<a href=\"https://github.com/MachineLearning100/100-Days-Of-ML-Code\">100-Days-Of-ML-Code</a>查看。有任何的建议或者意见欢迎在issue中提出~</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis",
   "language": "python",
   "name": "data_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
